import torch


class ActorLoss(torch.nn.Module):
    def __init__(self,
                 entropy_regularization_on=False,
                 trust_region_coeff = 1e-6,
                 alpha=1e-3):
        """
        Loss function for the actor.
        Args:
            alpha: entropy regularization parameter.
        """
        super(ActorLoss, self).__init__()
        self.entropy_regularization_on = entropy_regularization_on
        self.alpha = alpha

    def forward(self, Q, action_log_prob):
        """
        Computes the loss of the actor according to
        L = ð”¼_Ï€ [Q(a,s) - Î± log(Ï€(a|s)]
        Args:
            Q: Q(a,s)
            action_log_prob: log(Ï€(a|s)

        Returns:
            Scalar actor loss value
        """
        if self.entropy_regularization_on:
            return - (Q - self.alpha * action_log_prob).mean()
        else:
            return - Q.mean()

    def kl_divergence(self, old_mean, old_std, mean, std):
        """
        Computes:
        D_KL(Ï€_old(a|s)||Ï€(a|s)) = âˆ‘_i D_KL(Ï€_old(a_i|s)||Ï€(a_i|s))
        where Ï€(a_i|s) = N(a|Î¼, Ïƒ^2)
        and D_KL(Ï€_old(a_i|s)||Ï€(a_i|s)) = 1/2 * (log(Ïƒ^2/Ïƒ_old^2) + [Ïƒ_old^2 + (Î¼_old - Î¼)^2]/Ïƒ^2 -1)
        """
        t1 = torch.log(torch.pow(std, 2)) - torch.log(torch.pow(old_std, 2))
        t2 = (torch.pow(old_std, 2) + torch.pow(old_mean - mean, 2)) / torch.pow(std, 2)
        return torch.sum(t1 + t2 - 1, dim=-1) if mean.dim() > 2 else t1 + t2 - 1




